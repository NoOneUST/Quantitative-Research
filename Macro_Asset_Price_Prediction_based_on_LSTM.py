# Importing required packages
import yfinance as yf
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.ticker import FuncFormatter
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import root_mean_squared_error, mean_absolute_error
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings("ignore")

# Fetching S&P 500 index data
sp500 = yf.Ticker("^GSPC")

# Getting historical data for the S&P 500 index
sp500_hist = sp500.history(period="max")

# Printing earliest and latest dates in the fetched data
print("Earliest available data:", sp500_hist.index.min().date())
print("Most recent available data:", sp500_hist.index.max().date())

# Creating a complete date range
all_days = pd.date_range(start=sp500_hist.index.min(), end=sp500_hist.index.max(), freq='D')

# Resampling the data by creating a new DataFrame to hold the data and interpolation indications
sp500_hist_full = sp500_hist.reindex(all_days)

# Marking all NaN data points that will be generated by interpolation
sp500_hist_full['Interpolated'] = sp500_hist_full['Close'].isna()

# Interpolating, specifically the 'Close' column
sp500_hist_full['Close'].interpolate(method='time', inplace=True)

# Setting the style for matplotlib figures
plt.style.use('ggplot')

# Creating a new figure and setting the size
plt.figure(figsize=(16, 8))

# Plotting the closing prices and styling the plotted line
plt.plot(sp500_hist.index, sp500_hist['Close'], label='S&P 500 Close Price', color='#1f77b4', linewidth=1.5)

# Setting the titles, labels and font size of the graph
plt.title('S&P 500 Historical Close Price', fontsize=20, fontweight='bold')
plt.xlabel('Year', fontsize=16, fontweight='bold')
plt.ylabel('Close Price (USD)', fontsize=16, fontweight='bold')

# Setting up ticks on x-axis for better readability, and rotating date labels to avoid overlapping
plt.gca().xaxis.set_major_locator(mdates.YearLocator(10))
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
plt.xticks(rotation=45, ha='right')  # Setting alignment of x-axis labels

# Setting the font style for x-axis and y-axis labels
plt.xticks(fontsize=12, fontweight='bold')
plt.yticks(fontsize=12, fontweight='bold')

# Formatting y-axis to display prices with thousands separator
plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda x, p: format(int(x), ',')))

# Adding grid for better readability, and setting grid style
plt.grid(True, which='major', linestyle='--', linewidth=0.5, color='grey')

# Filling color to denote interpolated parts
plt.fill_between(sp500_hist_full.index, sp500_hist_full['Close'], where=sp500_hist_full['Interpolated'],
                 color='green', alpha=0.3, label='Interpolated Data')

# Displaying legend
plt.legend()

# Checking availability of GPU. It will use CUDA if available, else CPU will be used.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Getting current date-time and formatting it as a string
current_time = datetime.now().strftime('%Y%m%d-%H%M')
log_dir = f"runs/{current_time}"

# Initializing SummaryWriter for Tensorboard and writing logs to a subdirectory named after current time
writer = SummaryWriter(log_dir)

# Define LSTM model
class LSTMModel(nn.Module):
    # LSTMModel class inherits from torch.nn.Module, a base class for all neural network modules
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(LSTMModel, self).__init__()  # inherits properties from Module class
        self.hidden_dim = hidden_dim  # number of hidden layer dimensions
        self.num_layers = num_layers  # number of hidden layers
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)  # applies LSTM to input data
        self.linear = nn.Linear(hidden_dim, output_dim)  # applies linear transformation to incoming data

    # Defines the computation performed in every call to the model
    def forward(self, x):
        # initial hidden state for each element in the batch
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(x.device)
        # initial cell state
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().to(x.device)
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))  # passes the input and hidden states into the LSTM
        out = self.linear(out[:, -1, :])  # passes the last output tensor into the linear layer
        return out


# Function to create sequences
def create_sequences(data, seq_length):
    xs = []  # list to collect x sequences
    ys = []  # list to collect y sequences
    # iterate over data and create x, y sequences
    for i in range(len(data) - seq_length):
        x = data[i:(i + seq_length)]  # get x sequence
        y = data[i + seq_length]  # get y sequence
        xs.append(x)
        ys.append(y)
    # return arrays of x sequences and y sequences
    return np.array(xs), np.array(ys)


# Setup Hyperparameters
input_dim = 1  # input dimension
hidden_dim = 48  # number of hidden layer dimensions
num_layers = 2  # number of hidden layers
output_dim = 1  # output dimension
seq_length = 5  # length of the sequence
num_epochs = 10000  # number of epochs for training


# Data Preparation
# ---------------------------------------

# Fetch close prices of stocks and reshape for consistency
close_prices = sp500_hist_full['Close'].values.astype(float)
close_prices = close_prices.reshape(-1, 1)

# Get the dates for which the stocks data is available
dates = sp500_hist_full['Close'].index

# Normalize the data into a specified range for ease of processing
scaler = MinMaxScaler(feature_range=(-1, 1))
values_normalized = scaler.fit_transform(close_prices).reshape(-1)

# Create sequences out of the data for time-series analysis
X, y = create_sequences(values_normalized, seq_length)

# Divide data into training and validation sets
# ---------------------------------------
train_size = int(len(X) * 0.9)
X_train = X[:train_size]
y_train = y[:train_size]
X_val = X[train_size:]
y_val = y[train_size:]

# Convert data into PyTorch tensors for further processing
# ---------------------------------------
X_train = torch.FloatTensor(np.array(X_train)).view(-1, seq_length, input_dim).to(device)
y_train = torch.FloatTensor(np.array(y_train)).view(-1, output_dim).to(device)
X_val = torch.FloatTensor(np.array(X_val)).view(-1, seq_length, input_dim).to(device)
y_val = torch.FloatTensor(np.array(y_val)).view(-1, output_dim).to(device)

# Initialize the model, loss function and optimizer
# ---------------------------------------
model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)
criterion = torch.nn.MSELoss(reduction='mean')
# Use Mean Absolute Error (MAE) loss function for regression problem
mae_criterion = torch.nn.L1Loss(reduction='mean')
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Set a Cosine Annealing learning rate scheduler in the optimizer
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0)


# Training process
# Loop over each epoch for training
for epoch in range(num_epochs):
    model.train()  # Set the model to training mode
    optimizer.zero_grad()  # Zero the gradients before running the backward pass

    # Forward pass: compute predicted y by passing x to the model
    y_train_pred = model(X_train)

    loss = criterion(y_train_pred, y_train)  # Compute and print loss
    loss.backward()  # Compute gradient of the loss with respect to model parameters
    optimizer.step()  # Calling step function on an Optimizer makes an update to its parameters

    # Write training losses into TensorBoard
    loss_mae = mae_criterion(y_train_pred, y_train)
    loss_relative_error = (y_train_pred / y_train - 1).abs().mean()
    writer.add_scalar('Training loss_mse', loss.item(), epoch)
    writer.add_scalar('Training loss_mae', loss_mae.item(), epoch)
    writer.add_scalar('Training loss_mre', loss_relative_error.item(), epoch)

    # Periodically print loss
    if epoch % 10 == 0:
        print(f'Epoch {epoch} train loss: {loss.item()}')

    # Write current learning rate into TensorBoard
    writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], epoch)

    # Update learning rate
    scheduler.step()
# Close the writer
writer.close()

# Model Validation
# Switch model to evaluation mode
model.eval()

# Predict the validation set results
y_val_pred = model(X_val)

# Inverse the normalization to get original values
y_val_pred_abs = scaler.inverse_transform(y_val_pred.detach().cpu().numpy())
y_val_true_abs = scaler.inverse_transform(y_val.detach().cpu().numpy())
y_train_pred_abs = scaler.inverse_transform(y_train_pred.detach().cpu().numpy())
y_train_true_abs = scaler.inverse_transform(y_train.detach().cpu().numpy())

# Compute Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Relative Error (MRE)
# on the validation and train sets. These metrics provide us with measures of how well our model is performing.
val_rmse = root_mean_squared_error(y_val_true_abs, y_val_pred_abs)
val_mae = mean_absolute_error(y_val_true_abs, y_val_pred_abs)
val_relative = np.mean(np.abs(y_val_pred_abs / y_val_true_abs - 1))
train_rmse = root_mean_squared_error(y_train_true_abs, y_train_pred_abs)
train_mae = mean_absolute_error(y_train_true_abs, y_train_pred_abs)
train_relative = np.mean(np.abs(y_train_pred_abs / y_train_true_abs - 1))

# Print the RMSE, MAE, and MRE
print(f'Root Mean Squared Error (Validation): {val_rmse}')
print(f'Mean Absolute Error (Validation): {val_mae}')
print(f'Mean Relative Error (Validation): {val_relative}')
print(f'Root Mean Squared Error (Train): {train_rmse}')
print(f'Mean Absolute Error (Train): {train_mae}')
print(f'Mean Relative Error (Train): {train_relative}')

# Create a line plot of actual vs predicted values for our training set and validation set and enhances
# the plot with additional features like boundary lines and shaded regions.
# Setting the figure size
plt.figure(figsize=(10, 6))

# Creating date range for training and validation data
train_date_range = dates[seq_length: train_size + seq_length]
val_date_range = dates[train_size + seq_length:]

# Plotting the actual and predicted closing prices for train and validation set
plt.plot(train_date_range, y_train_true_abs, label='Actual Train', color='orange')
plt.plot(train_date_range, y_train_pred_abs, label='Predicted Train', color='red')
plt.plot(val_date_range, y_val_true_abs, label='Actual Validation', color='darkgreen')
plt.plot(val_date_range, y_val_pred_abs, label='Predicted Validation', color='blue')

# Define the boundary date as the last date in the training date range
boundary_date = train_date_range[-1]

# Draw a vertical line at the boundary date, making it evident where the training data ends and the validation data begins
plt.axvline(x=boundary_date, color='darkgreen', linestyle='--', lw=2, label='Train-Validation Boundary')

# Get the current x-axis limits for shading purposes
xlim_left, xlim_right = plt.xlim()

# Convert the boundary date from a Timestamp to a number recognized by matplotlib
boundary_date = mdates.date2num(boundary_date)

# Shade the area before the boundary in sky blue to represent the training data
plt.axvspan(xlim_left, boundary_date, color='skyblue', alpha=0.1)

# Shade the area after the boundary in light grey to represent the validation data
plt.axvspan(boundary_date, xlim_right, color='lightgrey', alpha=0.1)

# Give the plot a title and label the x and y axes
plt.title('S&P 500 Close Price Prediction using LSTM')
plt.xlabel('Date')
plt.ylabel('Close Price (USD)')

# Add a legend to the plot for clarification on the color coding
plt.legend()

# Display the final plot
plt.show()
